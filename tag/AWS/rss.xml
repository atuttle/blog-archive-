<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>github.com/atuttle/blog</title>
   
   <link>http://adamtuttle.codes</link>
   <description>It's one thing to write code. It's another to humanize technology so it serves a purpose in people's lives. &mdash; Luke Wroblewski</description>
   <language>en-us</language>
   <managingEditor> Adam Tuttle</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>My Experience With AWS Reserved EC2 Instances and Deploying Docker in Production</title>
	  <link>//AWS-Reserved-and-Docker-in-production/</link>
	  <author>Adam Tuttle</author>
	  <pubDate>2016-09-06T05:00:00-04:00</pubDate>
	  <guid>//AWS-Reserved-and-Docker-in-production/</guid>
	  <description><![CDATA[
	     <p>This is going to be more field-notes than how-to, but when I tweeted about how I had such an easy experience getting some Docker containers running in production I had several people express interest in reading about my experiences, so here we are.</p>

<p>The backstory is that I had one app in particular that was on a terrible, terrible web host (like really, the worst) &ndash;and it was for a paid client, at that&ndash; so when the host spent more time offline than online in June, and ignored my numerous support tickets, I decided to take my money elsewhere. The app happened to be a fairly simple website for a non-profit organization, with a members-only area, a way to pay dues and register for events, and a few other odds and ends. On the host-that-shall-remain-nameless this app was running on Adobe ColdFusion 10, and using a local MariaDB. I am not willing to pay the price Adobe charges for their AMIs long term, so I knew I had to convert to Lucee.</p>

<p>Having recently attended the <a href="http://www.devobjective.com/">dev.Objective()</a> conference and subsequently been <a href="http://adamtuttle.codes/TIL-adding-a-jvm-ssl-cert-docker/">playing with Docker</a> I decided I would use it to develop the necessary changes. It really made getting a development environment up and running terribly easy. In fact, I&#39;ve made it this far in my life without <em>properly</em> learning how to install and configure Tomcat, why start now?</p>

<p>And truly, that&#39;s the best part about all of this: <strong>Standing on the shoulders of giants.</strong> My most sincere thanks go to the Lucee team, in particular Geoff Bowers who manages their docker images. Also many thanks to the fine folks in the #lucee and #docker channels on the <a href="http://cfml-slack.herokuapp.com/">CFML Slack</a> who helped me through issues as they would come up.</p>

<p><video src="/assets/images/posts/2016/first-water-landing-720p.webm" controls>
    Sorry, your browser doesn't support embedded videos,
    but don't worry, you can <a href="/assets/images/posts/2016/first-water-landing-720p.webm">download it</a>
    and watch it with your favorite video player!
</video></p>

<p>Right, so after a month of evenings and weekends toiling away at the code changes, I got my local docker container for my app into good shape. I also managed to get a separate container up with MariaDB and network the two together. It worked, but it wasn&#39;t pretty. I had a series of shell scripts to stand it up and hold it together, but nothing I would want to use in production. Most importantly, I had no idea how I would make the DB survive a container restart (it was building from scratch on every start).</p>

<p>For the former problem (my poorly written shell scripts) <a href="https://docs.docker.com/compose/">docker-compose</a> came to the rescue, and for the latter (DB data storage) all I had to do was RTFM. Compose makes it easy to define a set of containers that need to be started and networked together, and manage them. And <a href="https://hub.docker.com/_/mariadb/">the MariaDB docker image docs</a> literally have a heading, &quot;Where to Store Data.&quot;</p>

<p>I&#39;m not going to spell out how those things work in detail, but here&#39;s my <code>docker-compose.yml</code> file:</p>
<div class="highlight"><pre><code class="language-yaml" data-lang="yaml"><span class="l-Scalar-Plain">version</span><span class="p-Indicator">:</span> <span class="s">&#39;2&#39;</span>
<span class="l-Scalar-Plain">services</span><span class="p-Indicator">:</span>
  <span class="l-Scalar-Plain">web</span><span class="p-Indicator">:</span>
    <span class="l-Scalar-Plain">container_name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">myapp_web</span>
    <span class="l-Scalar-Plain">restart</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">always</span>
    <span class="l-Scalar-Plain">build</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">./</span>
    <span class="l-Scalar-Plain">ports</span><span class="p-Indicator">:</span>
    <span class="p-Indicator">-</span> <span class="s">&quot;8888:80&quot;</span>
    <span class="l-Scalar-Plain">volumes</span><span class="p-Indicator">:</span>
<span class="hll">    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">./:/var/www/myapp</span>
</span>    <span class="l-Scalar-Plain">links</span><span class="p-Indicator">:</span>
<span class="hll">    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">db:myapp_db</span>
</span>    <span class="l-Scalar-Plain">environment</span><span class="p-Indicator">:</span>
        <span class="l-Scalar-Plain">TZ</span><span class="p-Indicator">:</span> <span class="s">&#39;America/New_York&#39;</span>
  <span class="l-Scalar-Plain">db</span><span class="p-Indicator">:</span>
    <span class="l-Scalar-Plain">container_name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">myapp_db</span>
    <span class="l-Scalar-Plain">restart</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">always</span>
    <span class="l-Scalar-Plain">image</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">mariadb:latest</span>
    <span class="l-Scalar-Plain">ports</span><span class="p-Indicator">:</span>
    <span class="p-Indicator">-</span> <span class="s">&quot;6633:3306&quot;</span>
    <span class="l-Scalar-Plain">volumes</span><span class="p-Indicator">:</span>
<span class="hll">    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">./database:/var/lib/mysql</span>
</span><span class="hll">    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">./res/create-database.sql:/docker-entrypoint-initdb.d/create-databse.sql</span>
</span><span class="hll">    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">./res/myapp.sql:/docker-entrypoint-initdb.d/myapp.sql</span>
</span>    <span class="l-Scalar-Plain">environment</span><span class="p-Indicator">:</span>
        <span class="l-Scalar-Plain">TZ</span><span class="p-Indicator">:</span> <span class="s">&#39;America/New_York&#39;</span>
        <span class="l-Scalar-Plain">MYSQL_ROOT_PASSWORD</span><span class="p-Indicator">:</span> <span class="s">&#39;godmode&#39;</span>
</code></pre></div>
<p>I&#39;ve highlighted a few of the interesting bits:</p>

<ul>
<li><code>./:/var/www/myapp</code> &mdash; mount the current directory into the into the container as a volume at <code>/var/www/myapp</code>, so that code changes I make on my laptop (because that&#39;s where I have my editor open) are immediately visible in the container.</li>
<li><code>db:myapp_db</code> &mdash; from inside the <strong>web</strong> container, make the <strong>db</strong> container available as hostname <code>myapp_db</code>

<ul>
<li>Not pictured: My Application.cfc has a datasource configured with the following connectionString: <code>&#39;jdbc:mysql://myapp_db:3306/myapp?...&#39;</code>, so you can see that you simply access the linked docker container by hostname.</li>
</ul></li>
<li>mount three volumes for the db container:

<ul>
<li><code>./database:/var/lib/mysql</code> uses the local <strong>database</strong> folder as the storage location for all of the data that MariaDB will create. (Starts out empty)</li>
<li><code>./res/create-database.sql:/docker-entrypoint-initdb.d/create-databse.sql</code> is a script that MariaDB will run on startup if it hasn&#39;t already created any databases (appears to be first-run). In this script I create my database and setup the website user account and privileges. After the first time the container is run, when there is data in the <strong>database</strong> folder, this file will be ignored.</li>
<li><code>./res/myapp.sql:/docker-entrypoint-initdb.d/myapp.sql</code> will run after <code>create-database.sql</code> and is just the most recent production mysqldump prepended with <code>use myapp;</code>. After the first time the container is run, when there is data in the <strong>database</strong> folder, this file will be ignored.</li>
</ul></li>
</ul>

<p>With those two problems solved, I moved on to thinking about how to manage all of this. I settled on a Makefile because I knew it would work both on my local Mac and on Ubuntu, where this will ultimately live. (Credit to <a href="http://www.compoundtheory.com/">Mark Mandel</a> for the idea to use Make in the first place. Smart dude.) Here&#39;s my Makefile:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="hll"><span class="nv">file_date</span> <span class="o">=</span> <span class="k">$(</span>shell date +%Y-%m-%d<span class="k">)</span>
</span>
default:
    docker ps

db-backup:
<span class="hll">    tar czf myapp_db.<span class="k">$(</span>file_date<span class="k">)</span>.tar.gz database
</span><span class="hll">    aws s3 cp myapp_db.<span class="k">$(</span>file_date<span class="k">)</span>.zip s3://myapp/backups/ <span class="o">&amp;&amp;</span> rm -f myapp_db.<span class="k">$(</span>file_date<span class="k">)</span>.zip
</span><span class="hll">    curl https://nosnch.in/my-secret-key
</span>
build:
    docker-compose up -d --build

up:
    docker-compose up -d

bash:
    docker <span class="nb">exec</span> -it myapp_web /bin/bash

logs:
    docker-compose logs -f
</code></pre></div>
<p>Probably the most interesting part of this is the <code>db-backup</code> target. This Makefile is to be used outside of the container, so what this target does is gzip up a copy of the <strong>database</strong> folder and push it into Amazon S3 using their <a href="https://aws.amazon.com/cli/">AWS CLI</a>. Lastly it notifies <a href="http://adamtuttle.codes/the-right-tool-success-notifications/">Dead Man&#39;s Snitch</a> that the backup has run so that I can sleep soundly. Should we ever need to restore the database, it should be as simple as unzipping the appropriate backup file into the database folder and starting the container.</p>

<p><img src="/assets/images/posts/2016/sit-flying-1.jpg" alt="Nginx welcome screen"></p>

<h3>All Problems Solved. Time to Go to Production!</h3>

<p>I spent a lot of time over the last month thinking about this. Despite being interested in Google&#39;s cloud offerings, I had already sold my client on AWS so I knew I was going to be hosting it there. But should I use their <a href="https://aws.amazon.com/ecs/">EC2 Container Service</a>? If not that, then what? I am aware of Kubernetes but suspect it would be easier to use on Google&#39;s cloud than Amazon&#39;s. Mostly this whole config was just a big question mark for me.</p>

<p>One thing I did know is that scaling is <em>just not an issue</em>. This app will only ever need one container and that&#39;s it. I don&#39;t need a cluster, I just need something that will restart it on the off chance the process dies. (That&#39;s what <code>restart: always</code> does in docker-compose, by the way.) So while it would be fun to run some sort of clustered setup, it would be the very definition of overkill.</p>

<p>For that reason, I decided to go with a standard vanilla Ubuntu AMI, on which I installed Git, Docker, and the AWS CLI. Because of the agreement I negotiated with my client, I knew I wanted a <code>3-year-reserved-full-upfront</code> EC2 instance, to save money over the on-demand prices. I&#39;ve used EC2 before, but never reserved instances, so that was another question mark. Like many things AWS, it makes sense once you understand how it works but was not immediately obvious without reading the documentation very carefully.</p>

<h4>Getting a Reserved EC2 Instance</h4>

<p>Reserved instances are kind of funny. When you buy it, you&#39;re not buying an EC2 instance: You&#39;re buying the contract to host an EC2 instance of a certain type, for a certain time period, in a certain location. For example, a <code>t2.large</code> for three years in <code>us-east-1d</code>. The clock starts ticking as soon as the payment form is submitted, so if it were to take a week for you to stand that instance up, you just donated a week&#39;s hosting costs to Amazon. Be ready to start that instance before you submit your payment so that you can switch tabs and start it immediately.</p>

<p>The Reserved Instance contracts (I&#39;m just calling them contracts, I don&#39;t know if there&#39;s a more official nomenclature) are sold on a sort of marketplace. I had heard that if you buy 3 years but end up wanting to terminate early, you&#39;re allowed to sell the remainder on the marketplace &mdash; I just didn&#39;t know how or where to do that. When I was searching for my contract, I only found one option to buy from &quot;3rd party&quot; rather than Amazon.</p>

<p>From there, it works just like any other EC2 instance: save your key file and then connect to the server with SSH: <code>ssh -i ~/.ssh/my_server-root.pem ubuntu@ec2-{my-ip-address}.compute-1.amazonaws.com</code>.</p>

<h3>Setting up My Server</h3>

<p>The first thing I needed to do was <a href="https://docs.docker.com/engine/installation/linux/ubuntulinux/">install Docker on Ubuntu</a>, which was pretty straight-forward:</p>

<p>All of the below commands are taken from the above Docker on Ubuntu link, after parsing out which commands were necessary for Ubuntu 14.04 Trusty. If you&#39;re on a different version, make sure to follow the right docs. And I&#39;m probably running <code>apt-get update</code> more than is strictly necessary, but it was repeated in the guide and I didn&#39;t see any harm in running it again, so I just followed instructions. (Standing on the shoulders of giants, remember?)</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>uname -r
3.13.0-95-generic
</code></pre></div>
<p>Great news! I&#39;m on a recent enough kernel!</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo apt-get update
<span class="nv">$ </span>sudo apt-get install apt-transport-https ca-certificates
<span class="nv">$ </span>sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
<span class="nv">$ </span>sudo <span class="nb">echo</span> <span class="s2">&quot;deb https://apt.dockerproject.org/repo ubuntu-trusty main&quot;</span> &gt; /etc/apt/sources.list.d/docker.list
<span class="nv">$ </span>sudo apt-get update
<span class="nv">$ </span>apt-cache policy docker-engine
<span class="nv">$ </span>sudo apt-get install linux-image-extra-<span class="k">$(</span>uname -r<span class="k">)</span> linux-image-extra-virtual
<span class="nv">$ </span>sudo apt-get install docker-engine
<span class="nv">$ </span>sudo service docker start
<span class="nv">$ </span>sudo docker run hello-world
</code></pre></div>
<p>Also from that guide:</p>

<blockquote>
<p>For [Ubuntu] 14.10 and below the above installation method automatically configures upstart to start the docker daemon on boot</p>
</blockquote>

<p>... Nice! One less thing to figure out. At this point I was thinking something like, &quot;So the Docker daemon will autostart, but how am I going to make sure my containers autostart, too?&quot; Spoiler alert: I restarted my instance with the containers running and when everything came back up they were brought up too. 👌</p>

<p>At this point I&#39;ve got Docker running but no docker-compose. Coming from a Mac I was surprised they didn&#39;t come bundled together. I was even more surprised that compose wasn&#39;t available through apt. Fortunately it was easy enough to google up some instructions on <a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-14-04">installing docker-compose on Ubuntu 14.04</a>.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="hll"><span class="nv">$ </span>sudo usermod -aG docker <span class="k">$(</span>whoami<span class="k">)</span>
</span><span class="nv">$ </span>sudo apt-get -y install python-pip
<span class="nv">$ </span>sudo pip install docker-compose
<span class="hll"><span class="nv">$ </span>which docker-compose <span class="o">&amp;&amp;</span> docker-compose --version
</span></code></pre></div>
<p>The <code>usermod</code> line adds the current user (ubuntu) to the <code>docker</code> group, which prevents the need to use sudo every time I want to run a docker command. The last line shows where the compose executable is found and prints its version, just to confirm that all is well.</p>

<p>And now to install the aforementioned AWS CLI:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo pip install awscli
</code></pre></div>
<h3>Configuring My App</h3>

<p>Now that all of the pre-requisites are satisfied, it&#39;s time to get my containers running and verify that my app is working. I cloned my git repo to <code>/opt/myapp</code> and then <code>chown -R ubuntu ubuntu /opt/myapp</code> so that sudo isn&#39;t required for simple things like getting the latest code with <code>git pull</code>.</p>

<p>I have both my <code>Dockerfile</code> for building my web container and my <code>docker-compose.yml</code> in the root of my repo, so it was as simple as <code>cd /opt/myapp &amp;&amp; docker-compose up</code> to start them.</p>

<p>Once the containers were done building, I confirmed that the web container was serving the website on port 8888. Internal to the container it&#39;s listening on port 80, but that port is being shared on the host as 8888, as you can see in my <code>docker-compose.yml</code> config above. From the host I make an HTTP request to localhost:8888:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>wget localhost:8888 -O /dev/null
Resolving localhost <span class="o">(</span>localhost<span class="o">)</span>... 127.0.0.1
Connecting to localhost <span class="o">(</span>localhost<span class="o">)</span><span class="p">|</span>127.0.0.1<span class="p">|</span>:8888... connected.
HTTP request sent, awaiting response... <span class="m">200</span> OK
Length: <span class="m">19789</span> <span class="o">(</span>19K<span class="o">)</span> <span class="o">[</span>text/html<span class="o">]</span>
Saving to: ‘/dev/null’

100%<span class="o">[==========================================</span>&gt;<span class="o">]</span> 19,789      --.-K/s   in 0s

2016-09-05 09:56:39 <span class="o">(</span><span class="m">173</span> MB/s<span class="o">)</span> - ‘/dev/null’ saved <span class="o">[</span>19789/19789<span class="o">]</span>
</code></pre></div>
<p>Awesome, it&#39;s working!</p>

<h4>Reverse Proxy</h4>

<p>This part isn&#39;t strictly necessary. If I only wanted to run this one thing on the server I could just directly expose the container&#39;s port 80 as the server&#39;s port 80 and call it a day. But putting a reverse-proxy in the middle allows me to add more containers to this server to do other things, which I hope to do in the near future. So I quickly wrapped it up in an <a href="https://www.digitalocean.com/community/tutorials/how-to-install-nginx-on-ubuntu-14-04-lts">Nginx</a> reverse proxy:</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>sudo apt-get install nginx
</code></pre></div>
<p>Once installed, Nginx is already running:</p>

<p><img src="/assets/images/posts/2016/nginx-welcome.png" alt="Nginx welcome screen"></p>

<p>I didn&#39;t spend a lot of time searching out the best Nginx Reverse Proxy tutorial; it was something that was pretty easy to figure out from just the hints in the <a href="https://www.digitalocean.com/community/tutorials/how-to-configure-nginx-as-a-web-server-and-reverse-proxy-for-apache-on-one-ubuntu-14-04-droplet">2nd google result</a> (jump down to &quot;Step 7&quot;).</p>

<p>I deleted the symlink at <code>/etc/nginx/sites-enabled/default</code> to disable the welcome screen, then added my own <code>/etc/nginx/sites-available/myapp.com</code> file with this content:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">server {
    listen 80 default_server;

    server_name myapp.com www.myapp.com aws.myapp.com;

    location / {
        proxy_pass http://127.0.0.1:8888;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
</code></pre></div>
<p>Then symlink this file into <code>/etc/nginx/sites-enabled/</code>, and restart nginx. This will proxy all requests to my container that&#39;s listening on port 8888, which is good enough for now!</p>

<h3>Remaining Problems</h3>

<p>Something&#39;s wonky with the system time inside the docker container. By default it comes up as UTC. My host was also UTC. I live in US/Eastern and my client is in US/Eastern so it just makes things easier for my servers to use US/Eastern, too. (Though really, <a href="https://youtu.be/-5wpm-gesOY">Timezones can die in a fire for all I care</a>.)</p>

<p>But I noticed this in the Lucee admin overview at around 22:33 (10:33pm) on Sept 4th:</p>

<p><img src="/assets/images/posts/2016/docker-lucee-time.png" alt="Nginx welcome screen"></p>

<p>What&#39;s odd is that it&#39;s ahead by ten hours (The container and the host were both in UTC at the time). I changed both to use Timezone <code>America/New_York</code> and restarted both the host and the containers, but that doesn&#39;t seem to have helped. Setting the Timezone in the MariaDB container seems to have worked: <code>select current_timestamp;</code> returns my current local time, so I have to imagine there&#39;s some disconnect between the system time and Lucee.</p>

<p>Just to verify I did add a comment to a view where I spit out the current value of <code>#now()#</code> and it matches what I see in the Lucee dashboard. So the website thinks that the time is off by a few hours. Not ideal, but I can live with it while I work with the folks in #lucee and #docker to find a fix.</p>

<p>I initially made some scripting changes to my Dockerfile to set the timezone and install NTP, but later found some advice to set an environment variable named <code>TZ</code> with the desired Timezone. That worked too (in that <code>$ date</code> reported the desired date &amp; time), so I ripped out the Dockerfile changes. The environment variable approach works for the MariaDB container too, an added bonus.</p>

<h3>Conclusion</h3>

<p>Soup to nuts, it went surprisingly smoothly. Aside from the Timezone/system time issue, I finished all of that in a day. I started at about 9:00am, took short breaks for lunch and dinner, and finished before 10:00pm. I had mentally prepared myself to stay up until the wee hours of the morning pulling out what&#39;s left of my hair, as I usually would in a similar situation, but it just wasn&#39;t necessary. The time that I saved, I turned around and used to write this blog post. 🤓</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I mean, it&#39;s also not even 10:00pm... This is just not how I envisioned my night going.<br><br>I&#39;ll allow it. <a href="https://t.co/iDwVd8PXgv">pic.twitter.com/iDwVd8PXgv</a></p>&mdash; Adam Tuttle (@AdamTuttle) <a href="https://twitter.com/AdamTuttle/status/772607180721750018">September 5, 2016</a></blockquote>

<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

	  ]]></description>
	</item>


</channel>
</rss>
